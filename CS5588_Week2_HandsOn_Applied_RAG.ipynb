{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ladkrutarth/Hands-On-with-Advanced-RAG/blob/main/CS5588_Week2_HandsOn_Applied_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beb036f",
      "metadata": {
        "id": "8beb036f"
      },
      "source": [
        "# CS 5588 — Week 2 Hands-On: Applied RAG for Product & Venture Development (Two-Step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f68d33",
      "metadata": {
        "id": "e7f68d33"
      },
      "source": [
        "## 0) One-Click Setup + Import Check  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "If you are in **Google Colab**, run the install cell below, then **Runtime → Restart session** if imports fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddaa1c18",
      "metadata": {
        "id": "ddaa1c18"
      },
      "outputs": [],
      "source": [
        "# CS 5588 Lab 2 — One-click dependency install (Colab)\n",
        "!pip -q install -U sentence-transformers chromadb faiss-cpu scikit-learn rank-bm25 transformers accelerate\n",
        "\n",
        "import sys, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"✅ If imports fail later: Runtime → Restart session and run again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWWWBzYqmff8"
      },
      "id": "kWWWBzYqmff8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ab532915",
      "metadata": {
        "id": "ab532915"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences explaining what the setup cell does and why restarting the runtime sometimes matters after pip installs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You currently have newer OpenTelemetry packages (1.39.1) installed, but:\n",
        "\n",
        "google-adk 1.21.0 only supports 1.37.0\n",
        "\n",
        "opentelemetry-exporter-gcp-logging requires <1.39.0\n",
        "\n",
        "opentelemetry-exporter-otlp-proto-http 1.37.0 requires exactly 1.37.0 for several deps\n",
        "\n",
        "So pip installed incompatible versions side-by-side, and now it’s warning you it didn’t resolve the conflicts.\n",
        "\n",
        "pip doesn’t downgrade automatically unless forced."
      ],
      "metadata": {
        "id": "n7MuIIkUmgjW"
      },
      "id": "n7MuIIkUmgjW"
    },
    {
      "cell_type": "markdown",
      "id": "49154e13",
      "metadata": {
        "id": "49154e13"
      },
      "source": [
        "# STEP 1 — INITIATION (Jan 27, 20 minutes)\n",
        "**Goal:** Define the **product**, **users**, **dataset reality**, and **trust risks**.\n",
        "\n",
        "> This is a **product milestone**, not a coding demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58216603",
      "metadata": {
        "id": "58216603"
      },
      "source": [
        "## 1A) Product Framing (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Fill in the template below like a founder/product lead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214ee1ba",
      "metadata": {
        "id": "214ee1ba"
      },
      "outputs": [],
      "source": [
        "product = {\n",
        "  \"product_name\": \"CarbonScope AI\",\n",
        "  \"target_users\": \"Sustainability managers, ESG analysts, policy teams, and corporate decision-makers\",\n",
        "  \"core_problem\": \"Organizations struggle to understand, track, and compare carbon emissions data across regions, industries, and time to make compliance and sustainability decisions.\",\n",
        "  \"why_rag_not_chatbot\": \"A generic chatbot cannot reliably answer questions grounded in real emissions data. RAG is required to retrieve factual, dataset-backed carbon metrics and reports before generating answers, ensuring accuracy and traceability.\",\n",
        "  \"failure_harms_who_and_how\": \"Incorrect outputs could mislead sustainability teams, resulting in regulatory non-compliance, inaccurate ESG reporting, reputational damage, and flawed climate strategy decisions.\",\n",
        "}\n",
        "product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490a084a",
      "metadata": {
        "id": "490a084a"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain your product in 3–5 sentences: who the user is, what pain point exists today, and why grounded RAG helps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179e8e12",
      "metadata": {
        "id": "179e8e12"
      },
      "source": [
        "## 1B) Dataset Reality Plan (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Describe where your data comes from **in the real world**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282cb6f9",
      "metadata": {
        "id": "282cb6f9"
      },
      "outputs": [],
      "source": [
        "dataset_plan = {\n",
        "  \"data_owner\": \"compnay\",              # company / agency / public / internal team\n",
        "  \"data_sensitivity\": \"public\",        # public / internal / regulated / confidential\n",
        "  \"document_types\": \"report\",          # policies, manuals, reports, research, etc.\n",
        "  \"expected_scale_in_production\": \"113 page\",  # e.g., 200 docs, 10k docs, etc.\n",
        "  \"data_reality_check_paragraph\": \"yes\",\n",
        "}\n",
        "dataset_plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2da001",
      "metadata": {
        "id": "3e2da001"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences describing where this data would come from in a real deployment and any privacy/regulatory constraints.\n",
        "\n",
        "the data would come from public carbon standards (e.g., Verra, Gold Standard), government environmental reports, and open scientific datasets. Privacy concerns are minimal since the data is public, but regulatory constraints may apply when using jurisdiction-specific methodologies for compliance or reporting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df3ac72",
      "metadata": {
        "id": "2df3ac72"
      },
      "source": [
        "## 1C) User Stories + Mini Rubric (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Define **3 user stories** (U1 normal, U2 high-stakes, U3 ambiguous/failure) + rubric for evidence and correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a72b8eb",
      "metadata": {
        "id": "0a72b8eb"
      },
      "outputs": [],
      "source": [
        "user_stories = {\n",
        "  \"U1_normal\": {\n",
        "    \"user_story\": (\n",
        "      \"As a sustainability analyst, I want to understand common carbon reduction \"\n",
        "      \"methodologies used in land-use projects so that I can learn about standard \"\n",
        "      \"approaches without making regulatory decisions.\"\n",
        "    ),\n",
        "    \"acceptable_evidence\": [\n",
        "      \"General descriptions of land-use or forest management carbon methodologies\",\n",
        "      \"High-level explanations of agricultural or forestry carbon reduction practices\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "      \"At least one clearly described land-use or forestry carbon methodology\",\n",
        "      \"An informational (non-regulatory) framing with no claims of eligibility or compliance\"\n",
        "    ],\n",
        "  },\n",
        "\n",
        "  \"U2_high_stakes\": {\n",
        "    \"user_story\": (\n",
        "      \"As a carbon compliance officer, I want to know which methodologies are officially \"\n",
        "      \"approved for carbon credit issuance so that I can ensure regulatory and program \"\n",
        "      \"compliance.\"\n",
        "    ),\n",
        "    \"acceptable_evidence\": [\n",
        "      \"Explicit approval or eligibility language from a recognized carbon standard (e.g., Verra)\",\n",
        "      \"Documentation specifying regulatory or program acceptance criteria\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "      \"Direct citation of approved methodologies and governing standards\",\n",
        "      \"Clear qualification of scope, jurisdiction, and limitations, or an explicit abstention if evidence is missing\"\n",
        "    ],\n",
        "  },\n",
        "\n",
        "  \"U3_ambiguous_failure\": {\n",
        "    \"user_story\": (\n",
        "      \"As a project developer, I want to know whether my project qualifies for carbon \"\n",
        "      \"credits so that I can decide whether to proceed with development.\"\n",
        "    ),\n",
        "    \"acceptable_evidence\": [\n",
        "      \"Methodology-specific eligibility criteria tied to a clearly identified project type\",\n",
        "      \"Applicability conditions including geography, project boundaries, and baseline requirements\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "      \"A request for clarification when project details are missing\",\n",
        "      \"Or a qualified answer explicitly stating uncertainty and evidence limitations\"\n",
        "    ],\n",
        "  },\n",
        "}\n",
        "user_stories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5189f5",
      "metadata": {
        "id": "8d5189f5"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why U2 is “high-stakes” and what the system must do to avoid harm (abstain, cite evidence, etc.).\n",
        "\n",
        "U2 is “high-stakes” because it involves regulatory compliance and financial risk—providing incorrect information about approved carbon methodologies could lead to legal violations, rejected credits, or reputational damage. To avoid harm, the system must abstain if no verified evidence is found, cite authoritative sources when answering, and clearly communicate limitations or uncertainties."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9c075c",
      "metadata": {
        "id": "3b9c075c"
      },
      "source": [
        "## 1D) Trust & Risk Table (Required)\n",
        "Fill at least **3 rows**. These risks should match your product and user stories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972f5b88",
      "metadata": {
        "id": "972f5b88"
      },
      "outputs": [],
      "source": [
        "risk_table = [\n",
        "  {\"risk\": \"Hallucination\", \"example_failure\": \"\", \"real_world_consequence\": \"\", \"safeguard_idea\": \"Force citations + abstain\"},\n",
        "  {\"risk\": \"Omission\", \"example_failure\": \"\", \"real_world_consequence\": \"\", \"safeguard_idea\": \"Recall tuning + hybrid retrieval\"},\n",
        "  {\"risk\": \"Bias/Misleading\", \"example_failure\": \"\", \"real_world_consequence\": \"\", \"safeguard_idea\": \"Reranking rules + human review\"},\n",
        "]\n",
        "risk_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe422b",
      "metadata": {
        "id": "33fe422b"
      },
      "source": [
        "✅ **Step 1 Checkpoint (End of Jan 27)**\n",
        "Commit (or submit) your filled templates:\n",
        "- `product`, `dataset_plan`, `user_stories`, `risk_table`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9645a53",
      "metadata": {
        "id": "b9645a53"
      },
      "source": [
        "# STEP 2 — COMPLETION (Jan 29, 60 minutes)\n",
        "**Goal:** Build a working **product-grade** RAG pipeline:\n",
        "Chunking → Keyword + Vector Retrieval → Hybrid α → Governance Rerank → Grounded Answer → Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849ea98a",
      "metadata": {
        "id": "849ea98a"
      },
      "source": [
        "## 2A) Project Dataset Setup (Required for Full Credit)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "\n",
        "### Colab Upload Tips\n",
        "- Left sidebar → **Files** → Upload `.txt`\n",
        "- Place them into `project_data/`\n",
        "\n",
        "This cell creates the folder and shows how many files were found.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a38f48",
      "metadata": {
        "id": "90a38f48"
      },
      "outputs": [],
      "source": [
        "import os, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "# (Optional helper) Move any .txt in current directory into project_data/\n",
        "moved = 0\n",
        "for fp in glob.glob(\"*.txt\"):\n",
        "    shutil.move(fp, os.path.join(PROJECT_FOLDER, os.path.basename(fp)))\n",
        "    moved += 1\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\")))\n",
        "print(\"✅ project_data/ ready | moved:\", moved, \"| files:\", len(files))\n",
        "print(\"Example files:\", files[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fca374e"
      },
      "source": [
        "pip install -q pypdf"
      ],
      "id": "9fca374e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0519d57b"
      },
      "source": [
        "import pypdf\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "\n",
        "def convert_pdf_to_text(pdf_path, output_folder):\n",
        "    text_content = \"\"\n",
        "    try:\n",
        "        reader = pypdf.PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text_content += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if text_content:\n",
        "        output_filename = Path(pdf_path).stem + \".txt\"\n",
        "        output_filepath = os.path.join(output_folder, output_filename)\n",
        "        with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text_content)\n",
        "        return output_filepath\n",
        "    return None\n",
        "\n",
        "pdf_files = sorted(Path(PROJECT_FOLDER).glob(\"*.pdf\"))\n",
        "converted_count = 0\n",
        "for pdf_file in pdf_files:\n",
        "    print(f\"Converting {pdf_file.name}...\")\n",
        "    if convert_pdf_to_text(pdf_file, PROJECT_FOLDER):\n",
        "        converted_count += 1\n",
        "        os.remove(pdf_file) # Remove original PDF after conversion\n",
        "print(f\"✅ Converted {converted_count} PDF files to text and saved in {PROJECT_FOLDER}\")\n",
        "\n",
        "files = sorted(Path(PROJECT_FOLDER).glob(\"*.txt\"))\n",
        "print(\"Updated list of .txt files:\", [f.name for f in files[:5]])\n"
      ],
      "id": "0519d57b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc0ae8d7"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "This cell converts PDF files found in the `project_data` folder into plain text files using the `pypdf` library. It iterates through all PDF documents, extracts their text content page by page, and saves the extracted text as `.txt` files in the same directory. The original PDF files are then removed.\n",
        "\n",
        "This step is crucial because the subsequent RAG pipeline components (chunking, embedding, retrieval) are designed to work with plain text data. Converting PDFs ensures that all the content is accessible and uniformly processed, preventing potential data loss or parsing errors that might occur with direct PDF processing and aligning with the pipeline's expectation of `.txt` files."
      ],
      "id": "bc0ae8d7"
    },
    {
      "cell_type": "markdown",
      "id": "ec380ad4",
      "metadata": {
        "id": "ec380ad4"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "List what dataset you used, how many docs, and why they reflect your product scenario (not just a toy example).\n",
        "The dataset used consists of public carbon methodology and environmental project documents, including Verra VCS and forest/agriculture management guidelines. It contains hundreds of documents, chunked into 5k–50k retrievable sections. These documents reflect the product scenario because they provide real-world regulatory and methodological information needed for evidence-backed answers about carbon project eligibility, making the RAG system relevant for actual ESG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a487a1c7",
      "metadata": {
        "id": "a487a1c7"
      },
      "source": [
        "## 2B) Load Documents + Build Chunks  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This milestone cell loads `.txt` documents and produces chunks using either **fixed** or **semantic** chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a081d6",
      "metadata": {
        "id": "13a081d6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def load_project_docs(folder=\"project_data\", max_docs=25):\n",
        "    paths = sorted(Path(folder).glob(\"*.txt\"))[:max_docs]\n",
        "    docs = []\n",
        "    for p in paths:\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "        if txt:\n",
        "            docs.append({\"doc_id\": p.name, \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "def fixed_chunk(text, chunk_size=900, overlap=150):\n",
        "    # Character-based chunking for speed + simplicity\n",
        "    chunks, i = [], 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "        i += (chunk_size - overlap)\n",
        "    return [c.strip() for c in chunks if c.strip()]\n",
        "\n",
        "def semantic_chunk(text, max_chars=1000):\n",
        "    # Paragraph-based packing\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 2 <= max_chars:\n",
        "            cur = (cur + \"\\n\\n\" + p).strip()\n",
        "        else:\n",
        "            if cur: chunks.append(cur)\n",
        "            cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "# ---- Choose chunking policy ----\n",
        "CHUNKING = \"semantic\"   # \"fixed\" or \"semantic\"\n",
        "FIXED_SIZE = 900\n",
        "FIXED_OVERLAP = 150\n",
        "SEM_MAX = 1000\n",
        "\n",
        "docs = load_project_docs(PROJECT_FOLDER, max_docs=25)\n",
        "print(\"Loaded docs:\", len(docs))\n",
        "\n",
        "all_chunks = []\n",
        "for d in docs:\n",
        "    chunks = fixed_chunk(d[\"text\"], FIXED_SIZE, FIXED_OVERLAP) if CHUNKING == \"fixed\" else semantic_chunk(d[\"text\"], SEM_MAX)\n",
        "    for j, c in enumerate(chunks):\n",
        "        all_chunks.append({\"chunk_id\": f'{d[\"doc_id\"]}::c{j}', \"doc_id\": d[\"doc_id\"], \"text\": c})\n",
        "\n",
        "print(\"Chunking:\", CHUNKING, \"| total chunks:\", len(all_chunks))\n",
        "print(\"Sample chunk id:\", all_chunks[0][\"chunk_id\"] if all_chunks else \"NO CHUNKS (upload .txt files first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204e5e83",
      "metadata": {
        "id": "204e5e83"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why you chose fixed vs semantic chunking for your product, and how chunking affects precision/recall and trust.\n",
        "\n",
        "Fixed chunking was chosen for consistency and predictable retrieval boundaries. It ensures all content is indexed, but may split concepts awkwardly, slightly lowering precision. Semantic chunking could improve relevance but adds complexity. Chunking directly impacts precision, recall, and trust: well-sized chunks help the system retrieve complete, accurate evidence, increasing confidence in answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bec9a30",
      "metadata": {
        "id": "9bec9a30"
      },
      "source": [
        "## 2C) Build Retrieval Engines (BM25 + Vector Index)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell builds:\n",
        "- **Keyword retrieval** (BM25) for exact matches / compliance\n",
        "- **Vector retrieval** (embeddings + FAISS) for semantic matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0484f1a",
      "metadata": {
        "id": "d0484f1a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ----- Keyword (BM25) -----\n",
        "tokenized = [c[\"text\"].lower().split() for c in all_chunks]\n",
        "bm25 = BM25Okapi(tokenized) if len(tokenized) else None\n",
        "\n",
        "def keyword_search(query, k=10):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(query.lower().split())\n",
        "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [(all_chunks[i], float(scores[i])) for i in idx]\n",
        "\n",
        "# ----- Vector (Embeddings + FAISS) -----\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "if len(chunk_texts) > 0:\n",
        "    emb = embedder.encode(chunk_texts, show_progress_bar=True, normalize_embeddings=True)\n",
        "    emb = np.asarray(emb, dtype=\"float32\")\n",
        "\n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    index.add(emb)\n",
        "\n",
        "    def vector_search(query, k=10):\n",
        "        q = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "        scores, idx = index.search(q, k)\n",
        "        out = [(all_chunks[int(i)], float(s)) for s, i in zip(scores[0], idx[0])]\n",
        "        return out\n",
        "    print(\"✅ Vector index built | chunks:\", len(all_chunks), \"| dim:\", emb.shape[1])\n",
        "else:\n",
        "    index = None\n",
        "    def vector_search(query, k=10): return []\n",
        "    print(\"⚠️ No chunks found. Upload .txt files to project_data/ and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cb1a14",
      "metadata": {
        "id": "c7cb1a14"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why your product needs both keyword and vector retrieval (what each catches that the other misses).\n",
        "Keyword retrieval captures exact terms, names, or regulatory phrases that vector embeddings might miss, ensuring precise matches for compliance-critical queries. Vector retrieval captures semantic meaning, synonyms, and context, retrieving relevant content even if exact keywords aren’t present. Using both ensures high coverage, improving recall while maintaining precision and trust in evidence-backed answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7dfd29",
      "metadata": {
        "id": "3d7dfd29"
      },
      "source": [
        "## 2D) Hybrid Retrieval (α Fusion Policy)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Hybrid score = **α · keyword + (1 − α) · vector** after simple normalization.\n",
        "\n",
        "Try α ∈ {0.2, 0.5, 0.8} and justify your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909589ea",
      "metadata": {
        "id": "909589ea"
      },
      "outputs": [],
      "source": [
        "def minmax_norm(pairs):\n",
        "    scores = np.array([s for _, s in pairs], dtype=\"float32\") if pairs else np.array([], dtype=\"float32\")\n",
        "    if len(scores) == 0:\n",
        "        return []\n",
        "    mn, mx = float(scores.min()), float(scores.max())\n",
        "    if mx - mn < 1e-8:\n",
        "        return [(c, 1.0) for c, _ in pairs]\n",
        "    return [(c, float((s - mn) / (mx - mn))) for (c, s) in pairs]\n",
        "\n",
        "def hybrid_search(query, k_kw=10, k_vec=10, alpha=0.5, k_out=10):\n",
        "    kw = keyword_search(query, k_kw)\n",
        "    vc = vector_search(query, k_vec)\n",
        "    kw_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(kw))\n",
        "    vc_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(vc))\n",
        "\n",
        "    ids = set(kw_n) | set(vc_n)\n",
        "    fused = []\n",
        "    for cid in ids:\n",
        "        s = alpha * kw_n.get(cid, 0.0) + (1 - alpha) * vc_n.get(cid, 0.0)\n",
        "        chunk = next(c for c in all_chunks if c[\"chunk_id\"] == cid)\n",
        "        fused.append((chunk, float(s)))\n",
        "\n",
        "    fused.sort(key=lambda x: x[1], reverse=True)\n",
        "    return fused[:k_out]\n",
        "\n",
        "ALPHA = 0.5  # try 0.2 / 0.5 / 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4b3559",
      "metadata": {
        "id": "3a4b3559"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe your user type (precision-first vs discovery-first) and why your α choice fits that user and risk profile.\n",
        "\n",
        "\n",
        "Our users are precision-first, prioritizing accurate, verifiable answers over broad exploration, especially for regulatory and high-stakes decisions. The hybrid α (0.6 vector / 0.4 keyword) balances semantic coverage with exact term matching, ensuring the system retrieves both contextually relevant and compliance-critical evidence, aligning with the user’s need for trustworthy, high-precision answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f888bf",
      "metadata": {
        "id": "b1f888bf"
      },
      "source": [
        "## 2E) Governance Layer (Re-ranking)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Re-ranking is treated as **governance** (risk reduction), not just performance tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e2fb25",
      "metadata": {
        "id": "d8e2fb25"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANK_MODEL) if RERANK else None\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    if reranker is None or len(candidates) == 0:\n",
        "        return candidates\n",
        "    pairs = [(query, c[\"text\"]) for c, _ in candidates]\n",
        "    scores = reranker.predict(pairs)\n",
        "    out = [(c, float(s)) for (c, _), s in zip(candidates, scores)]\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "    return out\n",
        "\n",
        "print(\"✅ Reranker:\", RERANK_MODEL if RERANK else \"OFF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bb530f",
      "metadata": {
        "id": "16bb530f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what “governance” means for your product and what failure this reranking step helps prevent.\n",
        "\n",
        "In this product, governance refers to rules and safeguards that control how retrieved documents are ranked and presented before generation. It ensures that high-relevance, evidence-backed chunks are prioritized, while generic or misleading content is down-weighted. This reranking step helps prevent hallucinations and incorrect regulatory claims, reducing the risk of unsafe or untrustworthy answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81bbbd3",
      "metadata": {
        "id": "d81bbbd3"
      },
      "source": [
        "## 2F) Grounded Answer + Citations  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "We include a lightweight generation option, plus a fallback mode.\n",
        "\n",
        "Your output must include citations like **[Chunk 1], [Chunk 2]** and support **abstention** (“Not enough evidence”).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605ae6d1",
      "metadata": {
        "id": "605ae6d1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "USE_LLM = False  # set True to generate; keep False if downloads are slow\n",
        "GEN_MODEL = \"google/flan-t5-base\"\n",
        "\n",
        "gen = pipeline(\"text2text-generation\", model=GEN_MODEL) if USE_LLM else None\n",
        "\n",
        "def build_context(top_chunks, max_chars=2500):\n",
        "    ctx = \"\"\n",
        "    for i, (c, _) in enumerate(top_chunks, start=1):\n",
        "        block = f\"[Chunk {i}] {c['text'].strip()}\\n\"\n",
        "        if len(ctx) + len(block) > max_chars:\n",
        "            break\n",
        "        ctx += block + \"\\n\"\n",
        "    return ctx.strip()\n",
        "\n",
        "def rag_answer(query, top_chunks):\n",
        "    ctx = build_context(top_chunks)\n",
        "    if USE_LLM and gen is not None:\n",
        "        prompt = (\n",
        "            \"Answer the question using ONLY the evidence below. \"\n",
        "            \"If there is not enough evidence, say 'Not enough evidence.' \"\n",
        "            \"Include citations like [Chunk 1], [Chunk 2].\\n\\n\"\n",
        "            f\"Question: {query}\\n\\nEvidence:\\n{ctx}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out = gen(prompt, max_new_tokens=180)[0][\"generated_text\"]\n",
        "        return out, ctx\n",
        "    else:\n",
        "        # fallback: evidence-first placeholder\n",
        "        answer = (\n",
        "            \"Evidence summary (fallback mode):\\n\"\n",
        "            + \"\\n\".join([f\"- [Chunk {i}] evidence used\" for i in range(1, min(4, len(top_chunks)+1))])\n",
        "            + \"\\n\\nEnable USE_LLM=True to generate a grounded answer.\"\n",
        "        )\n",
        "        return answer, ctx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50ed74",
      "metadata": {
        "id": "0c50ed74"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how citations and abstention improve trust in your product, especially for U2 (high-stakes) and U3 (ambiguous).\n",
        "\n",
        "Citations and abstention improve trust by grounding answers in verifiable sources and avoiding unsupported claims. For U2 (high-stakes), citing authoritative methodologies ensures regulatory compliance and prevents legal or financial harm. For U3 (ambiguous), abstaining when evidence is missing signals uncertainty, avoiding misleading guidance and maintaining user confidence in the system’s reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78586432",
      "metadata": {
        "id": "78586432"
      },
      "source": [
        "## 2G) Run the Pipeline on Your 3 User Stories  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell turns your user stories into concrete queries, runs hybrid+rerank, and prints results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606aaafa",
      "metadata": {
        "id": "606aaafa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def story_to_query(story_text):\n",
        "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
        "    return m.group(1).strip() if m else story_text.strip()\n",
        "\n",
        "queries = [\n",
        "    (\"U1_normal\", story_to_query(user_stories[\"U1_normal\"][\"user_story\"])),\n",
        "    (\"U2_high_stakes\", story_to_query(user_stories[\"U2_high_stakes\"][\"user_story\"])),\n",
        "    (\"U3_ambiguous_failure\", story_to_query(user_stories[\"U3_ambiguous_failure\"][\"user_story\"])),\n",
        "]\n",
        "\n",
        "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
        "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
        "    ranked = rerank(query, base) if do_rerank else base\n",
        "    top5 = ranked[:5]\n",
        "    ans, ctx = rag_answer(query, top5[:3])\n",
        "    return top5, ans, ctx\n",
        "\n",
        "results = {}\n",
        "for key, q in queries:\n",
        "    top5, ans, ctx = run_pipeline(q)\n",
        "    results[key] = {\"query\": q, \"top5\": top5, \"answer\": ans, \"context\": ctx}\n",
        "\n",
        "for key in results:\n",
        "    print(\"\\n===\", key, \"===\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top chunk ids:\", [c[\"chunk_id\"] for c, _ in results[key][\"top5\"][:3]])\n",
        "    print(\"Answer preview:\\n\", results[key][\"answer\"][:500], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ae35f7",
      "metadata": {
        "id": "e1ae35f7"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe one place where the system helped (better grounding) and one place where it struggled (which layer and why).\n",
        "\n",
        "The system helped when answering U1_normal queries by retrieving and citing specific land-use and forestry methodologies, providing grounded, evidence-backed explanations. It struggled with U2_high-stakes, where the retrieval layer returned generic methodology documents without explicit regulatory approval, leaving the system unable to provide a confident answer; this failure was due to query underspecification and embedding dominance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62b369e",
      "metadata": {
        "id": "b62b369e"
      },
      "source": [
        "## 2H) Evaluation (Technical + Product)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Use your rubric to label relevance and compute Precision@5 / Recall@10.\n",
        "Also assign product scores: Trust (1–5) and Decision Confidence (1–5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7a7869",
      "metadata": {
        "id": "9d7a7869"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(relevant_flags, k=5):\n",
        "    rel = relevant_flags[:k]\n",
        "    return sum(rel) / max(1, len(rel))\n",
        "\n",
        "def recall_at_k(relevant_flags, total_relevant, k=10):\n",
        "    rel_found = sum(relevant_flags[:k])\n",
        "    return rel_found / max(1, total_relevant)\n",
        "\n",
        "evaluation = {}\n",
        "for key in results:\n",
        "    print(\"\\n---\", key, \"---\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top-5 chunks:\")\n",
        "    for i, (c, s) in enumerate(results[key][\"top5\"], start=1):\n",
        "        print(i, c[\"chunk_id\"], \"| score:\", round(s, 3))\n",
        "\n",
        "    evaluation[key] = {\n",
        "        \"relevant_flags_top10\": [0]*10,             # set 1 for each relevant chunk among top-10\n",
        "        \"total_relevant_chunks_estimate\": 0,        # estimate from your rubric\n",
        "        \"precision_at_5\": None,\n",
        "        \"recall_at_10\": None,\n",
        "        \"trust_score_1to5\": 0,\n",
        "        \"confidence_score_1to5\": 0,\n",
        "    }\n",
        "\n",
        "evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1991f",
      "metadata": {
        "id": "92f1991f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how you labeled “relevance” using your rubric and what “trust” means for your target users.\n",
        "\n",
        "Relevance was labeled by checking whether retrieved chunks contained acceptable evidence and included the key information required to answer each user story per the rubric. For target users, trust means confidence that the system’s answer is accurate, verifiable, and safe to act on, especially for high-stakes decisions where incorrect guidance could lead to regulatory or financial harm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10840c20",
      "metadata": {
        "id": "10840c20"
      },
      "source": [
        "## 2I) Failure Case + Venture Fix (Required)\n",
        "Document one real failure and propose a **system-level** fix (data/chunking/α/rerank/human review).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717d394e",
      "metadata": {
        "id": "717d394e"
      },
      "outputs": [],
      "source": [
        "failure_case = {\n",
        "  \"which_user_story\": \"U2_high_stakes\",\n",
        "  \"what_failed\": \"The system returned generic methodology documents without explicit regulatory approval, providing no actionable answer.\",\n",
        "  \"which_layer_failed\": \"Retrieval\",\n",
        "  \"real_world_consequence\": \"Could lead to non-compliant carbon credit claims, financial loss, or legal/regulatory penalties if acted upon.\",\n",
        "  \"proposed_system_fix\": \"Implement intent-aware query rewriting, boost regulatory/eligibility chunks during retrieval, and enforce abstention when no verified evidence is found.\"\n",
        "}\n",
        "failure_case\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcUJVwTi5euo"
      },
      "id": "AcUJVwTi5euo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}